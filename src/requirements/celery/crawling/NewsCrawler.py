from typing import List, Union, Optional, Dict, Any
from .company import companys
from .NewsArticleCrawler import NewsArticleCrawler
from .utils import parse_datetime, extract_text
from playwright.sync_api import sync_playwright, Page, Browser, Playwright
import datetime
from time import sleep
from datetime import datetime
import asyncio
import time

class NewsCrawler(object):
	"""ê° ì¸ìŠ¤í„´ìŠ¤ê°€ ìì²´ Playwright ë¸Œë¼ìš°ì €ë¥¼ ê°€ì§€ëŠ” í¬ë¡¤ëŸ¬"""

	def __init__(self, company=None):
		"""ìƒì„±ìì—ì„œ Playwright ì´ˆê¸°í™”"""
		self.playwright = None
		self.browser = None
		self.page = None
		self.company = company

	def _init_driver(self):
		"""Playwright ì´ˆê¸°í™” (ì¸ìŠ¤í„´ìŠ¤ë³„ë¡œ)"""
		if self.browser is None:
			try:
				print(f"[{self.company}] Playwright ì´ˆê¸°í™” ì‹œì‘...")
				self.playwright = sync_playwright().start()
				print(f"[{self.company}] Playwright ì‹œì‘ë¨, ë¸Œë¼ìš°ì € ì‹¤í–‰ ì¤‘...")
				
				# íƒ€ì„ì•„ì›ƒê³¼ ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
				retry_count = 0
				max_retries = 3
				
				while retry_count < max_retries:
					try:
						self.browser = self.playwright.chromium.launch(
							headless=True,
							timeout=30000,  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
							args=[
								'--disable-gpu',
								'--disable-dev-shm-usage',
								'--disable-setuid-sandbox',
								'--no-sandbox',
								'--disable-extensions'
							]
						)
						break  # ì„±ê³µí•˜ë©´ ë£¨í”„ íƒˆì¶œ
					except Exception as e:
						retry_count += 1
						print(f"[{self.company}] ë¸Œë¼ìš°ì € ì‹œì‘ ì‹¤íŒ¨ ({retry_count}/{max_retries}): {str(e)}")
						if retry_count >= max_retries:
							raise  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ì‹œ ì˜ˆì™¸ ë°œìƒ
						time.sleep(2)  # ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
				
				print(f"[{self.company}] ë¸Œë¼ìš°ì € ì‹œì‘ë¨, í˜ì´ì§€ ìƒì„± ì¤‘...")
				self.page = self.browser.new_page(
					user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
					viewport={'width': 1280, 'height': 800}
				)
				print(f"[{self.company}] Playwright ì´ˆê¸°í™” ì™„ë£Œ")
			except Exception as e:
				print(f"[{self.company}] Playwright ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
				raise

	def _close_driver(self):
		"""Playwright ì¢…ë£Œ"""
		if self.page:
			self.page.close()
			self.page = None
		if self.browser:
			self.browser.close()
			self.browser = None
		if self.playwright:
			self.playwright.stop()
			self.playwright = None

	def _load_page(self, url: str, wait_selector: str = None):
		"""
		Playwrightë¡œ í˜ì´ì§€ ë¡œë“œ
		wait_selector: JavaScript ë¡œë”© ì™„ë£Œë¥¼ ê¸°ë‹¤ë¦´ ìš”ì†Œì˜ CSS ì„ íƒì
		"""
		try:
			print(f"[{self.company}] í˜ì´ì§€ ë¡œë“œ ì‹œì‘: {url}")
			self._init_driver()
			
			# í˜ì´ì§€ ë¡œë“œ ì‹œë„ (ìµœëŒ€ 5íšŒ ì¬ì‹œë„)
			retry_count = 0
			max_retries = 5
			base_wait_time = 3  # ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
			
			while retry_count < max_retries:
				try:
					# ìš”ì²­ ì‚¬ì´ì˜ ê°„ê²©ì„ ëœë¤í•˜ê²Œ ì¡°ì • (ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€)
					if retry_count > 0:
						# ì¬ì‹œë„í•  ë•Œë§ˆë‹¤ ëŒ€ê¸° ì‹œê°„ ì¦ê°€ (ì§€ìˆ˜ ë°±ì˜¤í”„)
						wait_time = base_wait_time * (2 ** retry_count) + (time.time() % 3)
						print(f"[{self.company}] ì¬ì‹œë„ {retry_count} - {wait_time:.1f}ì´ˆ ëŒ€ê¸° í›„ ìš”ì²­...")
						time.sleep(wait_time)
					
					# ì‚¬ìš©ì ì—ì´ì „íŠ¸ ëœë¤ ë³€ê²½
					user_agents = [
						'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
						'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
						'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15',
						'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
					]
					if retry_count > 0:
						# ì²« ì‹œë„ ì´í›„ì—ë§Œ ìƒˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
						if self.page:
							self.page.close()
						
						import random
						new_agent = random.choice(user_agents)
						print(f"[{self.company}] ìƒˆ ì‚¬ìš©ì ì—ì´ì „íŠ¸ ì‚¬ìš©: {new_agent[:20]}...")
						self.page = self.browser.new_page(
							user_agent=new_agent,
							viewport={'width': 1280, 'height': 800}
						)
					
					# í˜ì´ì§€ ë¡œë“œ
					response = self.page.goto(url, timeout=45000, wait_until='domcontentloaded')
					
					if not response:
						print(f"[{self.company}] ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {url}")
						retry_count += 1
						continue
						
					status = response.status
					print(f"[{self.company}] í˜ì´ì§€ ë¡œë“œ ì‘ë‹µ ì½”ë“œ: {status}")
					
					# 429 ì—ëŸ¬ ì²˜ë¦¬ (Too Many Requests)
					if status == 429:
						print(f"[{self.company}] âš ï¸ 429 ì—ëŸ¬: ë„ˆë¬´ ë§ì€ ìš”ì²­ - ë” ê¸´ ëŒ€ê¸° ì‹œê°„ ì ìš©")
						retry_count += 1
						# 429 ì—ëŸ¬ì˜ ê²½ìš° ë” ê¸´ ëŒ€ê¸° ì‹œê°„ ì ìš© (15-30ì´ˆ)
						wait_time = 15 + (retry_count * 5) + (time.time() % 5)
						print(f"[{self.company}] ë ˆì´íŠ¸ ë¦¬ë°‹ ëŒ€ê¸°: {wait_time:.1f}ì´ˆ...")
						time.sleep(wait_time)
						continue
					elif status >= 400:
						print(f"[{self.company}] í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ - HTTP ìƒíƒœ: {status}")
						retry_count += 1
						if retry_count >= max_retries:
							print(f"[{self.company}] ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {url}")
							break
						time.sleep(base_wait_time * (retry_count + 1))  # ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
						continue
					
					break  # ì„±ê³µí•˜ë©´ ë£¨í”„ íƒˆì¶œ
					
				except Exception as e:
					retry_count += 1
					print(f"[{self.company}] í˜ì´ì§€ ë¡œë“œ ì‹œë„ {retry_count}/{max_retries} ì‹¤íŒ¨: {str(e)}")
					if retry_count >= max_retries:
						print(f"[{self.company}] í˜ì´ì§€ ë¡œë“œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {url}")
						break
					time.sleep(2)  # ì ì‹œ ëŒ€ê¸° í›„ ì¬ì‹œë„
			
			# í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
			try:
				print(f"[{self.company}] ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì™„ë£Œ ëŒ€ê¸° ì¤‘...")
				self.page.wait_for_load_state("networkidle", timeout=15000)
				print(f"[{self.company}] í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {url}")
			except Exception as e:
				print(f"[{self.company}] ë„¤íŠ¸ì›Œí¬ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼, ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤: {str(e)}")
			
			# JavaScript ë™ì  ë¡œë”© ëŒ€ê¸°
			if wait_selector:
				try:
					print(f"[{self.company}] ì„ íƒì ëŒ€ê¸° ì¤‘: {wait_selector}")
					self.page.wait_for_selector(wait_selector, timeout=10000)
					sleep(1)  # ì¶”ê°€ ì•ˆì „ ëŒ€ê¸°
					print(f"[{self.company}] ì„ íƒì ê°ì§€ë¨: {wait_selector}")
				except Exception as e:
					print(f"[{self.company}] ì„ íƒì ëŒ€ê¸° ì¤‘ ì—ëŸ¬ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰): {str(e)}")
					
		except Exception as e:
			print(f"[{self.company}] í˜ì´ì§€ ë¡œë“œ ê³¼ì •ì—ì„œ ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)}")
			import traceback
			print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
			raise

	@staticmethod
	def crawl_sync(company: str) -> Optional[List[Dict[str, Any]]]:
		"""ë™ê¸°ì‹ í¬ë¡¤ë§ ë©”ì„œë“œ - ê°ê°ì˜ íšŒì‚¬ë§ˆë‹¤ ë…ë¦½ì ì¸ ì¸ìŠ¤í„´ìŠ¤ì™€ ë“œë¼ì´ë²„ ì‚¬ìš©"""
		print(f"[{company}] í¬ë¡¤ë§ ì‹œì‘ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
		crawler = NewsCrawler(company)  # íšŒì‚¬ë³„ë¡œ ë…ë¦½ëœ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
		try:
			if not NewsCrawler.check_company(company):
				raise ValueError("You should request one of limited company => \n \
'í•œêµ­ê²½ì œ', 'ì„¸ê³„ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 'ë¬¸í™”ì¼ë³´'")

			print(f"[{company}] í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì™„ë£Œ")
			result = []
			company_data = companys[company]
			domain = company_data.get('domain')
			items_count = company_data.get('items')
			# ì…€ë ‰í„°
			article_list_selector = company_data.get('article_list')
			categories = company_data.get('categories')
			print(f"[{company}] ì„¤ì • ë¡œë“œ ì™„ë£Œ: {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬, ë„ë©”ì¸: {domain}")

			# ì¹´í…Œê³ ë¦¬ ìš”ì²­ ì‚¬ì´ì˜ ëŒ€ê¸° ì‹œê°„ì„ ëœë¤í•˜ê²Œ ì„¤ì •í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
			import random

			for category, info in categories.items():
				# ì¹´í…Œê³ ë¦¬ ê°„ ëœë¤ ëŒ€ê¸° ì‹œê°„ ì ìš© (3-7ì´ˆ)
				category_wait = 3 + random.random() * 4
				print(f"[{company}] ì¹´í…Œê³ ë¦¬ '{category}' í¬ë¡¤ë§ ì‹œì‘ (ëŒ€ê¸° ì‹œê°„: {category_wait:.1f}ì´ˆ)")
				time.sleep(category_wait)

				for sub_category, sub_path in info['sub'].items():
					# í•˜ìœ„ ì¹´í…Œê³ ë¦¬ ê°„ ëœë¤ ëŒ€ê¸° ì‹œê°„ ì ìš© (1-3ì´ˆ)
					sub_category_wait = 1 + random.random() * 2
					print(f"[{company}] í•˜ìœ„ ì¹´í…Œê³ ë¦¬ '{sub_category}' í¬ë¡¤ë§ ì‹œì‘ (ëŒ€ê¸° ì‹œê°„: {sub_category_wait:.1f}ì´ˆ)")
					time.sleep(sub_category_wait)
					
					page_no = 1 - (company == 'ì„¸ê³„ì¼ë³´')
					is_today = True

					while is_today:
						# í˜ì´ì§€ ìš”ì²­ ì „ ì ì‹œ ëŒ€ê¸° (1-2ì´ˆ)
						page_wait = 1 + random.random()
						time.sleep(page_wait)
						
						print(f"[{company}] {category}-{sub_category} ì¹´í…Œê³ ë¦¬ì˜ page={page_no}")
						page_url = f"{domain}{info['path']}{sub_path}?page={page_no}"
						for item_idx in range(items_count):
							# í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
							crawler._load_page(page_url)
							sleep(2)
							# ê¸°ì‚¬ ëª©ë¡ ì ‘ê·¼
							article_list_elements = crawler.page.query_selector_all(article_list_selector)

							if not article_list_elements:
								print(f"{page_url} í˜ì´ì§€ì˜ CSS ì…€ë ‰í„° - {article_list_selector} HTML ìš”ì†Œë¥¼ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
								continue

							try:
								# ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•ŠëŠ”ì§€ í™•ì¸
								if item_idx < len(article_list_elements):
									item_element = article_list_elements[item_idx]
								else:
									print(f"ì¸ë±ìŠ¤ {item_idx}ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚©ë‹ˆë‹¤. ì´ {len(article_list_elements)}ê°œ í•­ëª©ì´ ìˆìŠµë‹ˆë‹¤.")
									continue
									
								# href ì†ì„± ê°€ì ¸ì˜¤ê¸°
								href = item_element.get_attribute('href')

								if not href:
									print(f"No href found at index {item_element}")
									continue

								# ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
								if href.startswith('/'):
									article_url = f"{domain}{href}"
								else:
									article_url = href

								print(f"Found article URL: {article_url}")
								
								# ê¸°ì‚¬ ìš”ì²­ ê°„ ëœë¤ ëŒ€ê¸° ì‹œê°„ ì ìš© (3-7ì´ˆ)
								import random
								article_wait = 3 + random.random() * 4
								print(f"[{company}] ê¸°ì‚¬ ì ‘ê·¼ ì „ {article_wait:.1f}ì´ˆ ëŒ€ê¸°...")
								sleep(article_wait)

								# ----------------------------------
								# ğŸ˜€ ì—¬ê¸°ì„œ NewsArticleCrawler í™œìš©!
								# ----------------------------------
								title, date, content = NewsArticleCrawler.crawl(company, article_url)

								if (title == "" or date == "" or content == ""):
									continue

								# ë‚ ì§œê°€ ì˜¤ëŠ˜ì¸ì§€ í™•ì¸
								today = datetime.now().date()
								article_date = None

								# ë¬¸ìì—´ í˜•íƒœì˜ ë‚ ì§œë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
								try:
									# date ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜ (utils.parse_datetime í•¨ìˆ˜ í™œìš©)
									article_date = parse_datetime(date).date() if date else None
								except Exception as e:
									print(f"ë‚ ì§œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")

								# ì˜¤ëŠ˜ ë‚ ì§œê°€ ì•„ë‹Œ ê²½ìš° ì¶œë ¥
								is_today = article_date == today if article_date else False
								if not is_today:
									print(f"âš ï¸ ì˜¤ëŠ˜ ë‚ ì§œ({today})ê°€ ì•„ë‹Œ ê¸°ì‚¬ì…ë‹ˆë‹¤: {article_date}")
									break

								# ê²°ê³¼ ê°ì²´ì— ì¶”ê°€
								article_data = {
									'title': title,
									'content': content,
									'category': category,
									'sub_category': sub_category,
									'published': date,
									'company': company,
									'news_url': article_url,
								}
								print(f"âœ… ì œëª©: {title}, ì‘ì„±ì¼ì: {date}, ê¸°ì‚¬ URL: {article_url}")
								result.append(article_data)

							except Exception as e:
								print(f"ê¸°ì‚¬ {page_url} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
								continue
						page_no += 1
			crawler._close_driver()
			return result

		except ValueError as v_err:
			print(v_err)
		except Exception as ex:
			print(f"í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬: {ex}")
		finally:
			# ë“œë¼ì´ë²„ ì¢…ë£Œ
			if (crawler):
				crawler._close_driver()

	@staticmethod
	def to_csv_sync(file_path: str, json_data: List[Dict[str, Any]]):
		"""ë™ê¸°ì‹ CSV ì‘ì„± ë©”ì„œë“œ"""
		import csv
		try:
			with open(file_path, mode='w', newline='', encoding="utf-8-sig") as file:
				writer = csv.writer(file)
				writer.writerow(['ì œëª©', 'ë³¸ë¬¸', 'ì¹´í…Œê³ ë¦¬', 'í•˜ìœ„ì¹´í…Œê³ ë¦¬', 'ê²Œì‹œì¼ì', 'ì‹ ë¬¸ì‚¬', 'ê¸°ì‚¬ë§í¬'])
				for data in json_data:
					writer.writerow(
						[
							data['title'],
							data['content'],
							data['category'],
							data['sub_category'],
							data['published'],
							data['company'],
							data['news_url']
						]
					)
				print(f"{file_path} ì‘ì„± ì™„ë£Œ!!")
		except Exception as ex:
			print(f"{file_path} ì‘ì„± ì‹¤íŒ¨: {ex}")
	
	@staticmethod
	def check_company(company: str) -> bool:
		if company in companys:
			return True
		return False

class AsyncNewsCrawler:
    """íšŒì‚¬ë³„ë¡œ ë³„ë„ì˜ ë“œë¼ì´ë²„ë¥¼ ì‚¬ìš©í•˜ëŠ” ë¹„ë™ê¸° í¬ë¡¤ëŸ¬"""
    
    @staticmethod
    async def crawl_company(company_name):
        """íšŒì‚¬ë³„ í¬ë¡¤ë§ ì‘ì—…ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰"""
        print(f"ğŸš€ {company_name} í¬ë¡¤ë§ ì‹œì‘...")
        
        try:
            # ë¹„ë™ê¸° ì‘ì—…ì„ ì´ë²¤íŠ¸ ë£¨í”„ì˜ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰
            # Playwright sync APIëŠ” ë¹„ë™ê¸°ê°€ ì•„ë‹ˆë¯€ë¡œ run_in_executorë¡œ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            loop = asyncio.get_running_loop()
            
            print(f"â³ {company_name}: ìŠ¤ë ˆë“œ í’€ ì‘ì—… ì‹œì‘...")
            result = await asyncio.wait_for(
                loop.run_in_executor(
                    None, 
                    lambda: NewsCrawler.crawl_sync(company_name)
                ),
                timeout=2700  # 45ë¶„ íƒ€ì„ì•„ì›ƒ ì„¤ì •
            )
            print(f"âœ“ {company_name}: ìŠ¤ë ˆë“œ í’€ í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ")
            
            if result:
                # CSV ì‘ì„±ë„ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰
                print(f"ğŸ“ {company_name}: CSV íŒŒì¼ ì‘ì„± ì‹œì‘...")
                await loop.run_in_executor(
                    None,
                    lambda: NewsCrawler.to_csv_sync(f"{company_name}.csv", result)
                )
                print(f"âœ… {company_name} í¬ë¡¤ë§ ë° CSV ì‘ì„± ì™„ë£Œ!")
            else:
                print(f"âš ï¸ {company_name}: í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¹ˆ ê²°ê³¼ì…ë‹ˆë‹¤.")
        
            return result
            
        except asyncio.TimeoutError:
            print(f"â›” {company_name}: í¬ë¡¤ë§ ì‘ì—… íƒ€ì„ì•„ì›ƒ (45ë¶„ ì´ˆê³¼)")
            return None
        except Exception as e:
            import traceback
            print(f"âŒ {company_name} í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return None

async def crawl_all_company_articles():
    """ëª¨ë“  íšŒì‚¬ì˜ í¬ë¡¤ë§ì„ ë™ì‹œì— ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰"""
    print("\n==========================================================")
    print(f"ğŸ“… í¬ë¡¤ë§ ì‘ì—… ì‹œì‘ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("==========================================================\n")
    
	# "ì¡°ì„ ì¼ë³´", "ì¤‘ì•™ì¼ë³´", "ë¬¸í™”ì¼ë³´"
    companys_name = ["í•œêµ­ê²½ì œ", "ì„¸ê³„ì¼ë³´", "ì¤‘ì•™ì¼ë³´", "ë¬¸í™”ì¼ë³´"]
    print(f"í¬ë¡¤ë§ ëŒ€ìƒ íšŒì‚¬: {', '.join(companys_name)}")
    
    start_time = time.time()
    
    try:
        # ê° íšŒì‚¬ë³„ë¡œ ë³„ë„ì˜ AsyncNewsCrawler ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¹„ë™ê¸° ì‹¤í–‰
        # gather ëŒ€ì‹  as_completed ì‚¬ìš©í•˜ì—¬ ì™„ë£Œë˜ëŠ” ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
        tasks = {
            asyncio.create_task(AsyncNewsCrawler.crawl_company(company_name), name=company_name): company_name
            for company_name in companys_name
        }
        
        results = []
        task_list = list(tasks.keys())
        
        print(f"ì´ {len(task_list)}ê°œ í¬ë¡¤ë§ ì‘ì—… ì‹œì‘ë¨")
        
        # ìµœëŒ€ 45ë¶„(2700ì´ˆ) íƒ€ì„ì•„ì›ƒ ì„¤ì •
        with_timeout = asyncio.wait_for(
            asyncio.gather(*task_list, return_exceptions=True),
            timeout=2700
        )
        
        all_results = await with_timeout
        
        # ê²°ê³¼ ì²˜ë¦¬ ë° ë¡œê¹…
        successful = 0
        for i, result in enumerate(all_results):
            company = companys_name[i]
            if isinstance(result, Exception):
                print(f"âŒ {company} í¬ë¡¤ë§ ì‹¤íŒ¨: {str(result)}")
            elif result:
                successful += 1
                print(f"âœ… {company} í¬ë¡¤ë§ ì„±ê³µ (ê¸°ì‚¬ ìˆ˜: {len(result)})")
            else:
                print(f"âš ï¸ {company} í¬ë¡¤ë§ ê²°ê³¼ ì—†ìŒ")
                
        results = all_results
    
    except asyncio.TimeoutError:
        print("â›” í¬ë¡¤ë§ ì „ì²´ ì‘ì—… ì‹œê°„ ì´ˆê³¼ (45ë¶„ ì´ˆê³¼)")
        results = []
    except Exception as e:
        import traceback
        print(f"âŒ í¬ë¡¤ë§ ì‘ì—… ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        results = []

    # ì‘ì—… ì™„ë£Œ ë¡œê¹…
    end_time = time.time()
    execution_time = end_time - start_time
    successful = sum(1 for r in results if r and not isinstance(r, Exception))

    print("\n==========================================================")
    print(f"ğŸ í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“Š ê²°ê³¼: ì´ {len(companys_name)}ê°œ íšŒì‚¬ ì¤‘ {successful}ê°œ í¬ë¡¤ë§ ì„±ê³µ")
    print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {execution_time:.2f}ì´ˆ")
    print("==========================================================\n")

    return results

if __name__ == '__main__':
    # ë¹„ë™ê¸° ì‹¤í–‰
    asyncio.run(crawl_all_company_articles())

