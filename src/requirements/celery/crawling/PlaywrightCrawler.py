from playwright.async_api import async_playwright
import asyncio
import time
from datetime import datetime
import pytz
import os
import csv

class PlaywrightNewsCrawler:
    """Playwrightë¥¼ ì‚¬ìš©í•˜ëŠ” ë¹„ë™ê¸° ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""

    @staticmethod
    async def crawl_company(company_name):
        """íšŒì‚¬ë³„ í¬ë¡¤ë§ ì‘ì—…ì„ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰"""
        print(f"ğŸš€ {company_name} í¬ë¡¤ë§ ì‹œì‘...")
        
        try:
            result = []
            async with async_playwright() as p:
                # ë¸Œë¼ìš°ì € ì‹¤í–‰
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                # íšŒì‚¬ë³„ ë„ë©”ì¸ ë° ì„¤ì •
                company_config = {
                    "í•œêµ­ê²½ì œ": {
                        "domain": "https://www.hankyung.com",
                        "categories": ["ê²½ì œ", "ì •ì¹˜"]
                    },
                    "ì„¸ê³„ì¼ë³´": {
                        "domain": "https://www.segye.com",
                        "categories": ["ê²½ì œ", "ì •ì¹˜"]
                    },
                    "ì¡°ì„ ì¼ë³´": {
                        "domain": "https://www.chosun.com",
                        "categories": ["ê²½ì œ", "ì •ì¹˜"]
                    },
                    "ì¤‘ì•™ì¼ë³´": {
                        "domain": "https://www.joongang.co.kr",
                        "categories": ["ê²½ì œ", "ì •ì¹˜"]
                    },
                    "ë¬¸í™”ì¼ë³´": {
                        "domain": "https://www.munhwa.com",
                        "categories": ["ê²½ì œ", "ì •ì¹˜"]
                    }
                }
                
                if company_name not in company_config:
                    print(f"âš ï¸ {company_name}ì— ëŒ€í•œ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
                    return None
                
                config = company_config[company_name]
                domain = config["domain"]
                
                # ì˜ˆì‹œ: íšŒì‚¬ ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸
                await page.goto(domain)
                await page.wait_for_load_state("networkidle")
                
                page_title = await page.title()
                print(f"ğŸ“° {company_name} ë©”ì¸ í˜ì´ì§€ ì œëª©: {page_title}")
                
                # ì‹¤ì œ í¬ë¡¤ë§ ë¡œì§ì€ ì—¬ê¸°ì— êµ¬í˜„
                # ì´ ì˜ˆì‹œì—ì„œëŠ” ê°„ë‹¨íˆ í˜ì´ì§€ ì œëª©ë§Œ ìˆ˜ì§‘
                
                result.append({
                    "title": page_title,
                    "content": "í¬ë¡¤ë§ ì˜ˆì‹œ ì½˜í…ì¸ ",
                    "category": "ì˜ˆì‹œ",
                    "sub_category": "í…ŒìŠ¤íŠ¸",
                    "published": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "company": company_name,
                    "news_url": domain,
                })
                
                # ë¸Œë¼ìš°ì € ì¢…ë£Œ
                await browser.close()
            
            # ê²°ê³¼ê°€ ìˆìœ¼ë©´ CSVë¡œ ì €ì¥
            if result:
                await PlaywrightNewsCrawler.save_to_csv(f"{company_name}.csv", result)
                print(f"âœ… {company_name} í¬ë¡¤ë§ ë° CSV ì‘ì„± ì™„ë£Œ!")
            
            return result
            
        except Exception as e:
            print(f"âŒ {company_name} í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return None

    @staticmethod
    async def save_to_csv(file_path, data):
        """ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(file_path, mode='w', newline='', encoding="utf-8-sig") as file:
                writer = csv.writer(file)
                writer.writerow(['ì œëª©', 'ë³¸ë¬¸', 'ì¹´í…Œê³ ë¦¬', 'í•˜ìœ„ì¹´í…Œê³ ë¦¬', 'ê²Œì‹œì¼ì', 'ì‹ ë¬¸ì‚¬', 'ê¸°ì‚¬ë§í¬'])
                for item in data:
                    writer.writerow([
                        item.get('title', ''),
                        item.get('content', ''),
                        item.get('category', ''),
                        item.get('sub_category', ''),
                        item.get('published', ''),
                        item.get('company', ''),
                        item.get('news_url', '')
                    ])
                print(f"{file_path} ì‘ì„± ì™„ë£Œ!!")
        except Exception as ex:
            print(f"{file_path} ì‘ì„± ì‹¤íŒ¨: {ex}")

async def crawl_all_companies():
    """ëª¨ë“  íšŒì‚¬ì˜ í¬ë¡¤ë§ì„ ë™ì‹œì— ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰"""
    companies = ["í•œêµ­ê²½ì œ", "ì„¸ê³„ì¼ë³´", "ì¡°ì„ ì¼ë³´", "ì¤‘ì•™ì¼ë³´", "ë¬¸í™”ì¼ë³´"]
    start_time = time.time()
    
    # ê° íšŒì‚¬ë³„ë¡œ ë³„ë„ë¡œ ë¹„ë™ê¸° ì‹¤í–‰
    tasks = [PlaywrightNewsCrawler.crawl_company(company) for company in companies]
    results = await asyncio.gather(*tasks)
    
    # ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ë©´
    end_time = time.time()
    successful = sum(1 for r in results if r)
    print(f"\nğŸ ì´ {len(companies)}ê°œ íšŒì‚¬ ì¤‘ {successful}ê°œ í¬ë¡¤ë§ ì™„ë£Œ!")
    print(f"âœ… ì´ ì†Œìš” ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
    
    return {
        "successful_crawls": successful,
        "total_companies": len(companies),
        "execution_time_seconds": end_time - start_time
    }